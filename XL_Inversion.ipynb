{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Inversion for Stable Diffusion XL 1.0\n",
    "**Copyright Â© 2023 [HANS ROETTGER](mailto:oss.roettger@posteo.org)**, distributed under the terms of **[AGPLv3](https://www.gnu.org/licenses/agpl-3.0.html)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path=\"../../DATA/T2I.Models/stability-AI/stable-diffusion-xl-base-1.0/\"\n",
    "\n",
    "# A single token to be used during the learning process; should NOT be used in \"prompts\" below\n",
    "learn_token=\"my\"\n",
    "# start learning with an embedding of single token or \"randn_like\" \n",
    "start_token=\"randn_like\"\n",
    "# list of learning rates [(#steps,learning_rate)] ; 4 gradient accumulation steps per step\n",
    "learning_rates=[(4,1e-3),(8,9e-4),(13,8e-4),(20,7e-4),(35,6e-4),(60,5e-4),(100,4e-4),(160,3e-4)]\n",
    "\n",
    "# Templates for training: {} defines the token to be learned (learn_token)\n",
    "template_prompts_for_objects=[\"a SLR photo of a {}\",\"a photo of a {}\",\"a rendering of a {}\",\"a cropped photo of a {}\",\"the rendering of a {}\",\"a photo of a small {}\",\"a photo of a fat {}\",\"a rendering of a dirty {}\",\"a dark photo of the {}\",\"a rendering of a big {}\",\"a 3D rendering of a {}\",\"a close-up photo of a {}\",\"a bright photo of the {}\",\"a cropped photo of a {}\",\"a rendering of the {}\",\"an award winning photo of a {}\",\"a photo of one {}\",\"a close-up photo of the {}\",\"a photo of the clean {}\",\"a rendering of a nice {}\",\"a good photo of a {}\",\"a full body photo of a cute {}\",\"a 3D rendering of the small {}\",\"a photo of the weird {}\",\"a photo of the large {}\",\"a rendering of a cool {}\",\"a SLR photo of a small {}\"]\n",
    "template_prompts_for_faces=[\"a color photo of {}\",\"a national geograhic photo of {}\",\"a national geograhic shot of {}\",\"a shot of {}\",\"a studio shot of {}\", \"a selfie of {}\",\"a SLR photo of {}\",\"a photo of {}\",\"a studio photo of {}\",\"a cropped photo of {}\",\"a close-up photo of {}\",\"an award winning photo of {}\",\"a good photo of {}\",\"a portrait photo of {}\",\"a portrait shot of {}\",\"a SLR photo of a cool {}\",\"a SLR photo of the face of {}\",\"a funny portrait of {}\",\"{}, portrait shot\",\"{}, studio lighting\",\"{}, bokeh\",\"{}, professional photo\"]\n",
    "template_prompts_for_styles=[\"a face in {} style\",\"a portrait, {}\",\"A {} portrait\",\"{} showing a face\",\"a portrait of a person depicted in a {}\",\"{} showing a person\",\"in style of {}\",\"person ,{} style\"]\n",
    "\n",
    "# Define prompts for training\n",
    "prompts=template_prompts_for_objects\n",
    "#prompts=template_prompts_for_faces\n",
    "#prompts=template_prompts_for_styles\n",
    "negative_prompt=\"deformed, ugly, disfigured, blurry, pixelated, hideous, indistinct, old, malformed, extra hands, extra arms, joined misshapen, collage, grainy, low, poor, monochrome, huge, extra fingers, mutated hands, cropped off, out of frame, poorly drawn hands, mutated hands, fused fingers, too many fingers, fused finger, closed eyes, cropped face, blur, long body, people, watermark, text, logo, signature, text, logo, writing, heading, no text, logo, wordmark, writing, heading, signature, 2 heads, 2 faces, b&w, nude, naked\"\n",
    "\n",
    "# prompt_variations (randomly added to {} in prompts) \n",
    "prompt_variations=[\"woman, white background\",\"figurine, white background\",\"doll, white background\"]\n",
    "#prompt_variations=[\", wearing white t-shirt, white background\"]\n",
    "#prompt_variations=[\"painting\",\"acryl\",\"art\",\"picture\"]\n",
    "\n",
    "# INPUT images\n",
    "imgs_path=\"./Images/Figure/\"\n",
    "#imgs_wh=(1024,1024) # 25 min for 500 steps (3090TI) -> noisy when used with lower INPUT image resolution\n",
    "imgs_wh=(768,768) # 15 min for 500 steps (3090TI) -> good results\n",
    "#imgs_wh=(512,512) # 10 min for 500 steps (3090TI) -> fastest\n",
    "imgs_flip=True # additionally use horizontally mirrored INPUT images\n",
    "\n",
    "# OUTPUT embedding\n",
    "embs_path=\"./Embeddings/\"\n",
    "emb_file=\"myPuppet768.pt\"\n",
    "\n",
    "# Visualize intermediate optimization steps\n",
    "test_prompt=\"a {} figurine at the beach\"\n",
    "intermediate_steps=9\n",
    "outGIF=\"./Samples/myPuppet768.gif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mitigate CUDA memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:50\"\n",
    "!echo $PYTORCH_CUDA_ALLOC_CONF\n",
    "#turn Xformers OFF while in training\n",
    "os.environ['FORCE_MEM_EFFICIENT_ATTN'] = \"0\"\n",
    "!echo $FORCE_MEM_EFFICIENT_ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline,DDPMScheduler\n",
    "import torch\n",
    "from PIL import Image,ImageEnhance\n",
    "import torchvision.transforms as T\n",
    "from tqdm import auto\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BASE model in bfloat16 dtype to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = DiffusionPipeline.from_pretrained(\n",
    "    base_model_path, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    variant=\"fp32\", \n",
    "    use_safetensors=False,\n",
    "    add_watermarker=False,\n",
    "    # use DDPM DDPMScheduler instead of default EulerDiscreteScheduler \n",
    "    scheduler = DDPMScheduler(num_train_timesteps=1000,prediction_type=\"epsilon\",beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False)\n",
    ")\n",
    "base.disable_xformers_memory_efficient_attention()\n",
    "torch.set_grad_enabled(True)\n",
    "_=base.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_training_grad(model,bT=True,bG=True):\n",
    "    model.training=bT\n",
    "    model.requires_grad_=bG\n",
    "    for module in model.children():\n",
    "        force_training_grad(module,bT,bG)\n",
    "        \n",
    "def load_imgs(path,wh=(1024,1024),flip=True,preview=(64,64)):\n",
    "    files=list()\n",
    "    imgs=list()\n",
    "    PILimgs=list()\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in [f for f in filenames if (f.endswith(\".jpg\") or f.endswith(\".JPG\") or f.endswith(\".png\") or f.endswith(\".JPEG\") or f.endswith(\".jpeg\"))]:\n",
    "            fname = os.path.join(dirpath, filename)\n",
    "            files.append(fname)\n",
    "    for f in files:\n",
    "        img = Image.open(f).convert(\"RGB\")\n",
    "        img = T.RandomAutocontrast(p=1.0)(img)\n",
    "        img = T.Resize(wh, interpolation=T.InterpolationMode.LANCZOS)(img)\n",
    "        #img = ImageEnhance.Contrast(T.RandomAutocontrast(p=1.0)(img)).enhance(5.0)\n",
    "        PILimgs.append(T.Resize(preview, interpolation=T.InterpolationMode.LANCZOS)(img))\n",
    "        img0 = T.ToTensor()(img)\n",
    "        img0 = img0 *2.- 1.0\n",
    "        imgs.append(img0[None].clip(-1.,1.))\n",
    "        # plus horizontally mirrowed\n",
    "        if flip:\n",
    "            img0 = T.RandomHorizontalFlip(p=1.0)(img0)  \n",
    "            imgs.append(img0[None].clip(-1.,1.)) \n",
    "            img = T.RandomHorizontalFlip(p=1.0)(img)\n",
    "            PILimgs.append(T.Resize(preview, interpolation=T.InterpolationMode.LANCZOS)(img))\n",
    "    return imgs,PILimgs\n",
    "\n",
    "def make_grid(imgs):\n",
    "    n=len(imgs)\n",
    "    cols=1\n",
    "    while cols*cols<n:\n",
    "        cols+=1\n",
    "    rows=n//cols+int(n%cols>0)\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))  \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for handling Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_XLembedding(emb,embedding_file=\"myToken.pt\",path=\"./Embeddings/\"):\n",
    "    torch.save(emb,path+embedding_file)\n",
    "\n",
    "def set_XLembedding(base,emb,token=\"my\"):\n",
    "    with torch.no_grad():            \n",
    "        # Embeddings[tokenNo] to learn\n",
    "        tokens=base.components[\"tokenizer\"].encode(token)\n",
    "        assert len(tokens)==3, \"token is not a single token in 'tokenizer'\"\n",
    "        tokenNo=tokens[1]\n",
    "        tokens=base.components[\"tokenizer_2\"].encode(token)\n",
    "        assert len(tokens)==3, \"token is not a single token in 'tokenizer_2'\"\n",
    "        tokenNo2=tokens[1]\n",
    "        embs=base.components[\"text_encoder\"].text_model.embeddings.token_embedding.weight\n",
    "        embs2=base.components[\"text_encoder_2\"].text_model.embeddings.token_embedding.weight\n",
    "        assert embs[tokenNo].shape==emb[\"emb\"].shape, \"different 'text_encoder'\"\n",
    "        assert embs2[tokenNo2].shape==emb[\"emb2\"].shape, \"different 'text_encoder_2'\"\n",
    "        embs[tokenNo]=emb[\"emb\"].to(embs.dtype).to(embs.device)\n",
    "        embs2[tokenNo2]=emb[\"emb2\"].to(embs2.dtype).to(embs2.device)\n",
    "\n",
    "def load_XLembedding(base,token=\"my\",embedding_file=\"myToken.pt\",path=\"./Embeddings/\"):\n",
    "    emb=torch.load(path+embedding_file)\n",
    "    set_XLembedding(base,emb,token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XL_textual_inversion(base,imgs,prompts,prompt_variations=None,token=\"my\",start_token=None,negative_prompt=None,learning_rates=[(5,1e-3),(10,9e-4),(20,8e-4),(35,7e-4),(55,6e-4),(80,5e-4),(110,4e-4),(145,3e-4)],intermediate_steps=9):\n",
    "    \n",
    "    XLt1=base.components[\"text_encoder\"]\n",
    "    XLt2=base.components[\"text_encoder_2\"]\n",
    "    XLtok1=base.components[\"tokenizer\"]\n",
    "    XLtok2=base.components[\"tokenizer_2\"]\n",
    "    XLunet=base.components[\"unet\"]\n",
    "    XLvae=base.components['vae']\n",
    "    XLsch=base.components['scheduler']\n",
    "    base.upcast_vae() # vae does not work correctly in 16 bit mode -> force fp32\n",
    "    \n",
    "    # Check Scheduler\n",
    "    schedulerType=XLsch.config.prediction_type\n",
    "    assert schedulerType in [\"epsilon\",\"sample\"], \"{} scheduler not supported\".format(schedulerType)\n",
    "\n",
    "    # Embeddings to Finetune\n",
    "    embs=XLt1.text_model.embeddings.token_embedding.weight\n",
    "    embs2=XLt2.text_model.embeddings.token_embedding.weight\n",
    "\n",
    "    with torch.no_grad():       \n",
    "        # Embeddings[tokenNo] to learn\n",
    "        tokens=XLtok1.encode(token)\n",
    "        assert len(tokens)==3, \"token is not a single token in 'tokenizer'\"\n",
    "        tokenNo=tokens[1]\n",
    "        tokens=XLtok2.encode(token)\n",
    "        assert len(tokens)==3, \"token is not a single token in 'tokenizer_2'\"\n",
    "        tokenNo2=tokens[1]            \n",
    "\n",
    "        # init Embedding[tokenNo] with noise or with a copy of an existing embedding\n",
    "        if start_token==\"randn_like\" or start_token==None:\n",
    "            # Original value range: [-0.5059,0.6538] # regular [-0.05,+0.05]\n",
    "            embs[tokenNo]=(torch.randn_like(embs[tokenNo])*.01).clone() # start with [-0.04,+0.04]\n",
    "            # Original value range 2: [-0.6885,0.1948] # regular [-0.05,+0.05]\n",
    "            embs2[tokenNo2]=(torch.randn_like(embs2[tokenNo2])*.01).clone() # start [-0.04,+0.04]\n",
    "            startNo=\"~\"\n",
    "            startNo2=\"~\"\n",
    "        else:  \n",
    "            tokens=XLtok1.encode(start_token)\n",
    "            assert len(tokens)==3, \"start_token is not a single token in 'tokenizer'\"\n",
    "            startNo=tokens[1]\n",
    "            tokens=XLtok2.encode(start_token)\n",
    "            assert len(tokens)==3, \"start_token is not a single token in 'tokenizer_2'\"\n",
    "            startNo2=tokens[1]\n",
    "            embs[tokenNo]=embs[startNo].clone()\n",
    "            embs2[tokenNo2]=embs2[startNo2].clone()\n",
    "\n",
    "        # Make a copy of all embeddings to keep all but the embedding[tokenNo] constant \n",
    "        index_no_updates = torch.arange(len(embs)) != tokenNo\n",
    "        orig=embs.clone()\n",
    "        index_no_updates2 = torch.arange(len(embs2)) != tokenNo2\n",
    "        orig2=embs2.clone()\n",
    " \n",
    "        print(\"Begin with '{}'=({}/{}) for '{}'=({}/{})\".format(start_token,startNo,startNo2,token,tokenNo,tokenNo2))\n",
    "\n",
    "        # Create all combinations [prompts] X [promt_variations]\n",
    "        if prompt_variations:\n",
    "            token=token+\" \"\n",
    "        else:\n",
    "            prompt_variations=[\"\"]            \n",
    "\n",
    "        txt_prompts=list()\n",
    "        for p in prompts:\n",
    "            for c in prompt_variations:\n",
    "                txt_prompts.append(p.format(token+c))\n",
    "        noPrompts=len(txt_prompts)\n",
    "        \n",
    "        # convert imgs to latents\n",
    "        samples=list()\n",
    "        for img in imgs:\n",
    "            samples.append(((XLvae.encode(img.to(XLvae.device)).latent_dist.sample(None))*XLvae.config.scaling_factor).to(XLunet.dtype)) # *XLvae.config.scaling_factor=0.13025:  0.18215    \n",
    "        noSamples=len(samples)\n",
    "           \n",
    "        # Training Parameters\n",
    "        batch_size=1\n",
    "        acc_size=4\n",
    "        total_steps=sum(i for i, _ in learning_rates)\n",
    "        # record_every_nth step is recorded in the progression list\n",
    "        record_every_nth=(total_steps//(intermediate_steps+1)+1)*acc_size\n",
    "        total_steps*=acc_size\n",
    "\n",
    "        # Prompt Parametrs\n",
    "        lora_scale = [0.6]  \n",
    "        time_ids = base._get_add_time_ids(list(imgs[0].shape[2:4]),[0,0],[1024,1024],dtype=XLunet.dtype).to(XLunet.device)\n",
    "\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        # Switch Models into training mode\n",
    "        force_training_grad(XLunet,True,True)\n",
    "        force_training_grad(XLt1,True,True)\n",
    "        force_training_grad(XLt2,True,True)\n",
    "        XLt1.text_model.train()\n",
    "        XLt2.text_model.train()\n",
    "        XLunet.train()\n",
    "        XLunet.enable_gradient_checkpointing()\n",
    "       \n",
    "        # Optimizer Parameters        \n",
    "        learning_rates=iter(learning_rates+[(0,0.0)]) #dummy for last update\n",
    "        sp,lr=next(learning_rates)\n",
    "        optimizer = torch.optim.AdamW([embs,embs2], lr=lr, betas=(0.9, 0.999), weight_decay=1e-2, eps=1e-8)   # 1e-7\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        # Progrssion List collects intermediate and final embedding\n",
    "        progression=list()\n",
    "        emb=embs[tokenNo].clone()\n",
    "        emb2=embs2[tokenNo2].clone()\n",
    "        progression.append({\"emb\":emb,\"emb2\":emb2})\n",
    "        \n",
    "        # Display [min (mean) max] of embeddings & current learning rate during training\n",
    "        desc=\"[{0:2.3f} ({1:2.3f}) +{2:2.3f}] [{3:2.3f} ({4:2.3f}) +{5:2.3f}] lr={6:1.6f}\".format(\n",
    "                        torch.min(emb.to(float)).detach().cpu().numpy(),\n",
    "                        torch.mean(emb.to(float)).detach().cpu().numpy(),\n",
    "                        torch.max(emb.to(float)).detach().cpu().numpy(),\n",
    "                        torch.min(emb2.to(float)).detach().cpu().numpy(),\n",
    "                        torch.mean(emb2.to(float)).detach().cpu().numpy(),\n",
    "                        torch.max(emb2.to(float)).detach().cpu().numpy(),\n",
    "                        lr)\n",
    "\n",
    "        # Training Loop\n",
    "        t=auto.trange(total_steps, desc=desc,leave=True)\n",
    "        for i in t:\n",
    "            # use random prompt, random time stepNo, random input image sample\n",
    "            prompt=txt_prompts[random.randrange(noPrompts)]\n",
    "            stepNo=torch.tensor(random.randrange(XLsch.config.num_train_timesteps)).unsqueeze(0).long().to(XLunet.device)\n",
    "            sample=samples[random.randrange(noSamples)].to(XLunet.device)\n",
    "\n",
    "            ### Target\n",
    "            noise = torch.randn_like(sample).to(XLunet.device)\n",
    "            target = noise\n",
    "            noised_sample=XLsch.add_noise(sample,noise,stepNo)\n",
    "\n",
    "            # Prediction\n",
    "            (prompt_embeds,negative_prompt_embeds,pooled_prompt_embeds,negative_pooled_prompt_embeds) = base.encode_prompt(\n",
    "                prompt=prompt,prompt_2=prompt,\n",
    "                negative_prompt=negative_prompt,negative_prompt_2=negative_prompt,\n",
    "                do_classifier_free_guidance=True,lora_scale=lora_scale)\n",
    "            cond_kwargs = {\"text_embeds\": pooled_prompt_embeds, \"time_ids\": time_ids}\n",
    "            pred = XLunet.forward(noised_sample,stepNo,prompt_embeds,added_cond_kwargs=cond_kwargs)['sample']\n",
    "                        \n",
    "            # Loss\n",
    "            loss = torch.nn.functional.mse_loss((pred).float(), (target).float(), reduction=\"mean\")                  \n",
    "            loss/=float(acc_size)\n",
    "            loss.backward() \n",
    "            \n",
    "            # One Optimization Step for acc_size gradient accumulation steps\n",
    "            if ((i+1)%acc_size)==0:\n",
    "                # keep Embeddings in normal value range\n",
    "                torch.nn.utils.clip_grad_norm_(XLt1.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(XLt2.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    # keep Embeddings for all other tokens stable      \n",
    "                    embs[index_no_updates]= orig[index_no_updates]\n",
    "                    embs2[index_no_updates2]= orig2[index_no_updates2]      \n",
    "                        \n",
    "                    # Current Embedding\n",
    "                    emb=embs[tokenNo].clone()        \n",
    "                    emb2=embs2[tokenNo2].clone()        \n",
    "                            \n",
    "                    if ((i+1)%(record_every_nth))==0:\n",
    "                        progression.append({\"emb\":emb,\"emb2\":emb2})\n",
    "                        \n",
    "                    # adjust learning rate?\n",
    "                    sp-=1\n",
    "                    if sp<1:\n",
    "                        sp,lr=next(learning_rates)\n",
    "                        for g in optimizer.param_groups:\n",
    "                            g['lr'] = lr\n",
    "                            \n",
    "                    # update display\n",
    "                    t.set_description(\"[{0:2.3f} ({1:2.3f}) +{2:2.3f}] [{3:2.3f} ({4:2.3f}) +{5:2.3f}] lr={6:1.6f}\".format(\n",
    "                        torch.min(emb.to(float)).detach().cpu().numpy(),\n",
    "                        torch.mean(emb.to(float)).detach().cpu().numpy(),\n",
    "                        torch.max(emb.to(float)).detach().cpu().numpy(),\n",
    "                        torch.min(emb2.to(float)).detach().cpu().numpy(),\n",
    "                        torch.mean(emb2.to(float)).detach().cpu().numpy(),\n",
    "                        torch.max(emb2.to(float)).detach().cpu().numpy(),\n",
    "                        lr))\n",
    "\n",
    "        # append final Embedding\n",
    "        progression.append({\"emb\":emb,\"emb2\":emb2})\n",
    "        \n",
    "        return progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,PILimgs=load_imgs(imgs_path,wh=imgs_wh,flip=imgs_flip)\n",
    "\n",
    "overview=make_grid(PILimgs)\n",
    "display(overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(46)\n",
    "progression=XL_textual_inversion(base,imgs=imgs,prompts=prompts,prompt_variations=prompt_variations,token=learn_token,start_token=start_token,negative_prompt=negative_prompt,learning_rates=learning_rates,intermediate_steps=intermediate_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final embedding\n",
    "save_XLembedding(progression[-1],embedding_file=emb_file,path=embs_path)\n",
    "# save intermediate embeddings\n",
    "save_XLembedding(progression,embedding_file=\"all\"+emb_file,path=embs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VAE was used in fp32 for training - switch back to fp16\n",
    "base.vae.to(base.unet.dtype)\n",
    "\n",
    "progression=torch.load(embs_path+\"all\"+emb_file)\n",
    "\n",
    "prompt=test_prompt.format(learn_token)\n",
    "seed=1\n",
    "\n",
    "frames=list()\n",
    "for emb in progression:\n",
    "    set_XLembedding(base,emb,token=learn_token)\n",
    "    with torch.no_grad():    \n",
    "        torch.manual_seed(seed)\n",
    "        image = base(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=40,\n",
    "            guidance_scale=7.5\n",
    "        ).images\n",
    "    frames.append(image[0])\n",
    "    display(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave(outGIF, frames+[frames[-1]]*2, format='GIF', duration=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
